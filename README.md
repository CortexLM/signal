<div align="center">

# **📡 Signal - Model Inference Subnet**
Advancing Optimized, Multi-Model Inference on the Bittensor Network

[![Discord Chat](https://img.shields.io/discord/308323056592486420.svg)](https://discord.gg/bittensor)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

[Discord](https://discord.gg/bittensor) • [Network](https://taostats.io/) • [Research](https://bittensor.com/whitepaper)

</div>

---

## 📑 Table of Contents

1. [🚀 Introduction](#introduction)
2. [✨ Key Features](#key-features)
   - [🔗 Dynamic Multi-Model Interoperability](#dynamic-multi-model-interoperability)
   - [⚙️ Adaptive LoRA Loading](#adaptive-lora-loading)
   - [📈 Scalable Optimization](#scalable-optimization)
3. [💻 Compute Requirements](#compute-requirements)
   - [💾 Disk Space](#disk-space)
   - [🧠 Memory](#memory)
   - [🖥️ CPU](#cpu)
4. [🔧 Getting Started](#getting-started)
5. [📥 Installation Guide](#installation-guide)
6. [🤝 Contributing](#contributing)
7. [📜 License](#license)

---

## 🚀 Introduction

**Signal** is an advanced Bittensor subnet designed to optimize the deployment of multi-model inference, supporting dynamic adaptation and resource allocation. Signal enables the streamlined execution of large-scale models across a distributed network, enhancing efficiency and scalability for a new generation of AI inference tasks.

## ✨ Key Features

### 🔗 Dynamic Multi-Model Interoperability
Signal supports the simultaneous operation of multiple models, optimizing computational resources and enabling high-precision inference.

### ⚙️ Adaptive LoRA Loading
With dynamic Low-Rank Adaptation (LoRA) loading, Signal can seamlessly adjust to the unique demands of diverse models in real time.

### 📈 Scalable Optimization
Signal’s architecture is tailored for large-scale, distributed AI tasks, facilitating efficient, high-performance model execution.

## 💻 Compute Requirements

Signal requires certain hardware specifications to operate efficiently. The requirements will vary depending on the scale of models deployed and the intended network throughput.

### 💾 Disk Space
- **Minimum**: 200 GB (suitable for smaller model deployments or limited scale).
- **Recommended**: 400 GB or more, especially if you intend to serve multiple, complex models simultaneously.
- **Note**: Disk space usage scales with the number and size of models you wish to deploy. Allocating more space enhances storage for additional models and enables faster access to large datasets.

### 🧠 Memory
- **Requirement**: 64 GB RAM.
- **Usage**: This amount of memory ensures smooth operation of dynamic model loading and efficient handling of concurrent model inferences. Higher memory allocations can improve performance with larger or more complex model architectures.

### 🖥️ CPU
- **Requirement**: 16 virtual CPUs (vCPUs).
- **Purpose**: Multi-threaded, high-throughput processing across a distributed setup. More vCPUs improve concurrent request handling and reduce processing latency.

## 🔧 Getting Started

Detailed steps on how to initiate your setup with Signal for multi-model inference can be found [here](#getting-started).

## 📥 Installation Guide

For a complete installation walkthrough, please refer to [Installation Guide](#installation-guide).

## 🤝 Contributing

We welcome contributions! See our [Contributing](#contributing) section for guidelines on how to get involved.

## 📜 License

Licensed under the MIT License. See the full license [here](https://opensource.org/licenses/MIT).
